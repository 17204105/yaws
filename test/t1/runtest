#!/bin/sh


#
# Load Yaws with a number of concurrent curl requests for files of varying
# size up to the fd limit to make sure the sendfile driver is not serializing
# requests and creating a request queue so long that it eats up all the available
# fds. The file sizes are passed into this script as command-line arguments.
#

sizes="$@"
fdlimit=`ulimit -n`
fdlimit=`expr $fdlimit / 4`
if [ `uname -s` = Darwin ]; then
    proclimit=`ulimit -u`
    if [ $proclimit -lt $fdlimit ]; then
        newproclimit=`expr $fdlimit \* 3`
        ulimit -u $newproclimit
        if [ $? -ne 0 ]; then
            echo unable to run enough processes for this test, skipping
            echo please raise your max process limit via '"sysctl -w kern.maxprocperuid=<N>"'
            exit 0
        fi
    fi
fi
outdir=./runtest$$
trap "rm -rf $outdir" HUP INT EXIT
mkdir -p $outdir
i=0
echo starting $fdlimit background curl tasks
while [ $i -lt $fdlimit ]; do
    for j in $sizes; do
        curl -s -o $outdir/${j}-${i}.dat http://localhost:8000/$j.dat >/dev/null 2>&1 &
        curls="$curls $!"
    done
    i=`expr $i + 1`
done
echo waiting for curl tasks to complete
for job in $curls; do
    wait $job
    result=$?
    if [ $result -ne 0 ]; then
        echo failure in background curl job $job, exit status was $result
        exit 1
    fi
done
echo verifying all retrieved files
for j in $sizes; do
    count=`ls -1 $outdir/${j}*.dat | wc -l`
    if [ $count -ne $fdlimit ]; then
        echo not all $j byte data files retrieved: expected $fdlimit, got $count
        exit 1
    fi
    none=`ls -l $outdir/${j}*.dat | awk '{print $5}' | grep -v $j`
    if [ -n "$none" ]; then
        echo not all $j byte data files have the right size
        exit 1
    fi
done

exit 0
